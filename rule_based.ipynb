{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def assign_grade(gender, race_ethnicity, parental_education, lunch, test_preparation_course):\n",
    "    # Identify high performers first\n",
    "    if parental_education == \"master's degree\" and test_preparation_course == 'completed':\n",
    "        return 'Superior'\n",
    "    \n",
    "    if parental_education == \"bachelor's degree\" and test_preparation_course == 'completed':\n",
    "        return 'Excellent'\n",
    "\n",
    "    if parental_education in [\"associate's degree\", \"some college\"] and test_preparation_course == 'completed':\n",
    "        return 'Good'\n",
    "\n",
    "    # If conditions for good performance are not met, default to Failure\n",
    "    return 'Failure'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "csv_path = \"D:\\student-classification-model\\graded_exams.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Ensure that we don't modify original data\n",
    "df = df.copy()\n",
    "\n",
    "# Features (excluding actual grade columns)\n",
    "features = df.drop(columns=['math grade', 'reading grade', 'writing grade'])\n",
    "\n",
    "# Target variables (actual dataset grades)\n",
    "target_math = df['math grade']\n",
    "target_reading = df['reading grade']\n",
    "target_writing = df['writing grade']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data (keeping your original split strategy)\n",
    "def split_data(features, target_math, target_reading, target_writing):\n",
    "    # First split: Training (70%) and Temp (30%)\n",
    "    X_train, X_temp, y_train_math, y_temp_math, y_train_reading, y_temp_reading, y_train_writing, y_temp_writing = train_test_split(\n",
    "        features, target_math, target_reading, target_writing, test_size=0.3, random_state=42\n",
    "    )\n",
    "\n",
    "    # Second split: Testing (20%) and Unseen (10%) from Temp (30%)\n",
    "    X_test, X_unseen, y_test_math, y_unseen_math, y_test_reading, y_unseen_reading, y_test_writing, y_unseen_writing = train_test_split(\n",
    "        X_temp, y_temp_math, y_temp_reading, y_temp_writing, test_size=1/3, random_state=42\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        X_train, X_test, X_unseen,\n",
    "        y_train_math, y_test_math, y_unseen_math,\n",
    "        y_train_reading, y_test_reading, y_unseen_reading,\n",
    "        y_train_writing, y_test_writing, y_unseen_writing\n",
    "    )\n",
    "\n",
    "# Apply split\n",
    "X_train, X_test, X_unseen, y_train_math, y_test_math, y_unseen_math, y_train_reading, y_test_reading, y_unseen_reading, y_train_writing, y_test_writing, y_unseen_writing = split_data(features, target_math, target_reading, target_writing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Math Grade Prediction (Rule-Based Classifier)\n",
      "Accuracy: 0.52\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Above Average       0.00      0.00      0.00         7\n",
      "      Average       0.00      0.00      0.00        13\n",
      "Below Average       0.00      0.00      0.00        19\n",
      "    Excellent       0.12      0.17      0.14         6\n",
      "      Failure       0.66      0.81      0.73       121\n",
      "         Good       0.10      0.44      0.16         9\n",
      "      Passing       0.00      0.00      0.00        19\n",
      "     Superior       0.00      0.00      0.00         6\n",
      "\n",
      "     accuracy                           0.52       200\n",
      "    macro avg       0.11      0.18      0.13       200\n",
      " weighted avg       0.41      0.52      0.45       200\n",
      "\n",
      "\n",
      "Reading Grade Prediction (Rule-Based Classifier)\n",
      "Accuracy: 0.46\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Above Average       0.00      0.00      0.00        19\n",
      "      Average       0.00      0.00      0.00        20\n",
      "Below Average       0.00      0.00      0.00        19\n",
      "    Excellent       0.25      0.33      0.29         6\n",
      "      Failure       0.58      0.81      0.68       107\n",
      "         Good       0.05      0.29      0.09         7\n",
      "      Passing       0.00      0.00      0.00        16\n",
      "     Superior       0.00      0.00      0.00         6\n",
      "\n",
      "     accuracy                           0.46       200\n",
      "    macro avg       0.11      0.18      0.13       200\n",
      " weighted avg       0.32      0.46      0.38       200\n",
      "\n",
      "\n",
      "Writing Grade Prediction (Rule-Based Classifier)\n",
      "Accuracy: 0.50\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Above Average       0.00      0.00      0.00         8\n",
      "      Average       0.00      0.00      0.00        15\n",
      "Below Average       0.00      0.00      0.00        25\n",
      "    Excellent       0.25      0.25      0.25         8\n",
      "      Failure       0.63      0.84      0.72       112\n",
      "         Good       0.10      0.36      0.16        11\n",
      "      Passing       0.00      0.00      0.00        14\n",
      "     Superior       0.00      0.00      0.00         7\n",
      "\n",
      "     accuracy                           0.50       200\n",
      "    macro avg       0.12      0.18      0.14       200\n",
      " weighted avg       0.37      0.50      0.42       200\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xaise\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Xaise\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Xaise\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Xaise\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Xaise\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Xaise\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Xaise\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Xaise\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\Xaise\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# **Apply Rule-Based Classification only to X_test**\n",
    "y_pred_math = X_test.apply(lambda row: assign_grade(\n",
    "    row[\"gender\"], \n",
    "    row[\"race/ethnicity\"], \n",
    "    row[\"parental level of education\"], \n",
    "    row[\"lunch\"], \n",
    "    row[\"test preparation course\"]), axis=1)\n",
    "\n",
    "y_pred_reading = X_test.apply(lambda row: assign_grade(\n",
    "    row[\"gender\"], \n",
    "    row[\"race/ethnicity\"], \n",
    "    row[\"parental level of education\"], \n",
    "    row[\"lunch\"], \n",
    "    row[\"test preparation course\"]), axis=1)\n",
    "\n",
    "y_pred_writing = X_test.apply(lambda row: assign_grade(\n",
    "    row[\"gender\"], \n",
    "    row[\"race/ethnicity\"], \n",
    "    row[\"parental level of education\"], \n",
    "    row[\"lunch\"], \n",
    "    row[\"test preparation course\"]), axis=1)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_rule_based(y_true, y_pred, target_name):\n",
    "    print(f\"{target_name} Grade Prediction (Rule-Based Classifier)\")\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()\n",
    "\n",
    "# Evaluate the rule-based classifier against ACTUAL grades\n",
    "evaluate_rule_based(y_test_math, y_pred_math, \"Math\")\n",
    "evaluate_rule_based(y_test_reading, y_pred_reading, \"Reading\")\n",
    "evaluate_rule_based(y_test_writing, y_pred_writing, \"Writing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seaborn style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create scatter plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Scatter plot for Math vs Reading\n",
    "sns.scatterplot(x=shuffled_data[\"math score\"], y=shuffled_data[\"reading score\"],\n",
    "                hue=shuffled_data[\"math grade\"], palette=\"coolwarm\", ax=axes[0])\n",
    "axes[0].set_title(\"Math vs. Reading Scores\")\n",
    "axes[0].set_xlabel(\"Math Score\")\n",
    "axes[0].set_ylabel(\"Reading Score\")\n",
    "\n",
    "# Scatter plot for Math vs Writing\n",
    "sns.scatterplot(x=shuffled_data[\"math score\"], y=shuffled_data[\"writing score\"],\n",
    "                hue=shuffled_data[\"math grade\"], palette=\"coolwarm\", ax=axes[1])\n",
    "axes[1].set_title(\"Math vs. Writing Scores\")\n",
    "axes[1].set_xlabel(\"Math Score\")\n",
    "axes[1].set_ylabel(\"Writing Score\")\n",
    "\n",
    "# Scatter plot for Reading vs Writing\n",
    "sns.scatterplot(x=shuffled_data[\"reading score\"], y=shuffled_data[\"writing score\"],\n",
    "                hue=shuffled_data[\"reading grade\"], palette=\"coolwarm\", ax=axes[2])\n",
    "axes[2].set_title(\"Reading vs. Writing Scores\")\n",
    "axes[2].set_xlabel(\"Reading Score\")\n",
    "axes[2].set_ylabel(\"Writing Score\")\n",
    "\n",
    "# Adjust layout and show plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate unseen data\n",
    "def evaluate_on_unseen(y_true, y_pred, subject):\n",
    "    print(f\"\\n{subject} Evaluation on Unseen Data\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_true, y_pred):.2f}\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y_true), yticklabels=np.unique(y_true))\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(f\"Confusion Matrix for {subject}\")\n",
    "    plt.show()\n",
    "\n",
    "# Apply rule-based classifier with categorical columns\n",
    "y_unseen_math = shuffled_data.apply(lambda row: assign_grade(row[\"math score\"], row[\"lunch\"], row[\"test preparation course\"], row[\"parental level of education\"]), axis=1)\n",
    "y_unseen_reading = shuffled_data.apply(lambda row: assign_grade(row[\"reading score\"], row[\"lunch\"], row[\"test preparation course\"], row[\"parental level of education\"]), axis=1)\n",
    "y_unseen_writing = shuffled_data.apply(lambda row: assign_grade(row[\"writing score\"], row[\"lunch\"], row[\"test preparation course\"], row[\"parental level of education\"]), axis=1)\n",
    "\n",
    "\n",
    "# Evaluate on unseen dataset\n",
    "evaluate_on_unseen(y_unseen_math, y_unseen_math, \"Math\")\n",
    "evaluate_on_unseen(y_unseen_reading, y_unseen_reading, \"Reading\")\n",
    "evaluate_on_unseen(y_unseen_writing, y_unseen_writing, \"Writing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
