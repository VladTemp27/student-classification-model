{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# Fix the file path to use proper escape characters\n",
    "csv_path = r\"graded_exams.csv\"  # Using raw string to avoid escape sequence issues\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Ensure that we don't modify original data\n",
    "df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features (excluding actual grade columns)\n",
    "features = df.drop(columns=['math grade', 'reading grade', 'writing grade'])\n",
    "\n",
    "# Target variables\n",
    "target_math = df['math grade']\n",
    "target_reading = df['reading grade']\n",
    "target_writing = df['writing grade']\n",
    "\n",
    "# Reset indices to avoid potential index mismatches\n",
    "features.reset_index(drop=True, inplace=True)\n",
    "target_math.reset_index(drop=True, inplace=True)\n",
    "target_reading.reset_index(drop=True, inplace=True)\n",
    "target_writing.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(features, target_math, target_reading, target_writing):\n",
    "    # First split: Training (70%) and Temp (30%)\n",
    "    X_train, X_temp, y_train_math, y_temp_math, y_train_reading, y_temp_reading, y_train_writing, y_temp_writing = train_test_split(\n",
    "        features, target_math, target_reading, target_writing, test_size=0.3, random_state=42\n",
    "    )\n",
    "\n",
    "    # Second split: Testing (20%) and Unseen (10%) from Temp (30%)\n",
    "    X_test, X_unseen, y_test_math, y_unseen_math, y_test_reading, y_unseen_reading, y_test_writing, y_unseen_writing = train_test_split(\n",
    "        X_temp, y_temp_math, y_temp_reading, y_temp_writing, test_size=1/3, random_state=42\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        X_train, X_test, X_unseen,\n",
    "        y_train_math, y_test_math, y_unseen_math,\n",
    "        y_train_reading, y_test_reading, y_unseen_reading,\n",
    "        y_train_writing, y_test_writing, y_unseen_writing\n",
    "    )\n",
    "\n",
    "# Apply split\n",
    "X_train, X_test, X_unseen, y_train_math, y_test_math, y_unseen_math, y_train_reading, y_test_reading, y_unseen_reading, y_train_writing, y_test_writing, y_unseen_writing = split_data(features, target_math, target_reading, target_writing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def analyze_feature_combinations(X_data, y_data, features_to_analyze, min_samples=10):\n",
    "    \"\"\"\n",
    "    This function enhances a rule-based classifier by identifying common feature \n",
    "    combinations that lead to specific grade outcomes. Similar to how **Apriori** is used in \n",
    "    association rule mining to discover frequent itemsets and generate rules from transaction data, \n",
    "    `analyze_feature_combinations` also identifies patterns, but with a focus on classification.\n",
    "\n",
    "    The function works by grouping the data based on selected features and determining the most \n",
    "    frequently occurring grade for each feature combination. This process helps uncover patterns \n",
    "    that strongly correlate with specific grades, much like how Apriori identifies frequent itemsets. \n",
    "    By associating these feature combinations with the most common grade, the function builds a \n",
    "    structured lookup table that aids in classification. This lookup table acts like a decision rule \n",
    "    system for the classifier, mapping feature combinations to the most common grade observed for each \n",
    "    combination, along with the distribution of grades.\n",
    "\n",
    "    While **Apriori** generates itemsets and association rules, which are typically used for uncovering \n",
    "    relationships in transactional data, `analyze_feature_combinations` creates decision rules in a \n",
    "    classification context. The key difference is that **Apriori** is focused on finding item associations \n",
    "    in the dataset, whereas `analyze_feature_combinations` is specifically designed to improve a \n",
    "    **rule-based classifier** by associating feature combinations with outcome predictions, like grades. \n",
    "    This enhances the classifier's ability to make predictions by forming structured, human-readable \n",
    "    decision rules that can be easily interpreted and applied.\n",
    "\n",
    "    In summary, while both methods identify patterns in data, **Apriori** is used for association rule mining \n",
    "    and itemset discovery, and `analyze_feature_combinations` enhances rule-based classification by creating \n",
    "    a structured decision-making framework based on feature combinations.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # Create a copy of X_data and add the target variable\n",
    "    data = X_data.copy()\n",
    "    data['grade'] = y_data  # Attach the target variable for analysis\n",
    "\n",
    "    # Group dataset by the selected features\n",
    "    grouped = data.groupby(features_to_analyze)\n",
    "\n",
    "    for name, group in grouped:\n",
    "        # Ensure only meaningful samples are considered\n",
    "        if len(group) >= min_samples:\n",
    "            # Count occurrences of each grade\n",
    "            grade_counts = Counter(group['grade'])\n",
    "            \n",
    "            # Identify the most frequently occurring grade\n",
    "            if grade_counts:\n",
    "                most_common_grade = grade_counts.most_common(1)[0][0]\n",
    "                \n",
    "                # Store the results, ensuring consistency in key format\n",
    "                if isinstance(name, tuple):\n",
    "                    results[name] = (most_common_grade, grade_counts)\n",
    "                else:\n",
    "                    results[(name,)] = (most_common_grade, grade_counts)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedRuleClassifier:\n",
    "    def __init__(self, subject):\n",
    "        self.subject = subject\n",
    "        self.rules = []\n",
    "        self.default_grade = None\n",
    "        \n",
    "    def build(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Build a hierarchy of rules based on the training data\n",
    "        \"\"\"\n",
    "        # First level: Parental education + test prep\n",
    "        education_test_prep_rules = analyze_feature_combinations(\n",
    "            X_train, y_train, \n",
    "            ['parental level of education', 'test preparation course'],\n",
    "            min_samples=10\n",
    "        )\n",
    "        \n",
    "        # Second level: Parental education + test prep + gender\n",
    "        education_test_prep_gender_rules = analyze_feature_combinations(\n",
    "            X_train, y_train, \n",
    "            ['parental level of education', 'test preparation course', 'gender'],\n",
    "            min_samples=8\n",
    "        )\n",
    "        \n",
    "        # Third level: Parental education + test prep + lunch\n",
    "        education_test_prep_lunch_rules = analyze_feature_combinations(\n",
    "            X_train, y_train, \n",
    "            ['parental level of education', 'test preparation course', 'lunch'],\n",
    "            min_samples=8\n",
    "        )\n",
    "        \n",
    "        # Fourth level: Parental education + test prep + race/ethnicity\n",
    "        education_test_prep_race_rules = analyze_feature_combinations(\n",
    "            X_train, y_train, \n",
    "            ['parental level of education', 'test preparation course', 'race/ethnicity'],\n",
    "            min_samples=5\n",
    "        )\n",
    "        \n",
    "        # Store rules in order of specificity (most specific first)\n",
    "        self.rules = [\n",
    "            (education_test_prep_race_rules, ['parental level of education', 'test preparation course', 'race/ethnicity']),\n",
    "            (education_test_prep_lunch_rules, ['parental level of education', 'test preparation course', 'lunch']),\n",
    "            (education_test_prep_gender_rules, ['parental level of education', 'test preparation course', 'gender']),\n",
    "            (education_test_prep_rules, ['parental level of education', 'test preparation course'])\n",
    "        ]\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict grades using the rule hierarchy\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for _, instance in X.iterrows():\n",
    "            prediction = None\n",
    "            \n",
    "            # Try each rule level\n",
    "            for rule_set, features in self.rules:\n",
    "                if prediction:\n",
    "                    break\n",
    "                \n",
    "                feature_values = tuple(instance[feature] for feature in features)\n",
    "                \n",
    "                if feature_values in rule_set:\n",
    "                    prediction = rule_set[feature_values][0]\n",
    "            \n",
    "            # Use default grade if no rules match\n",
    "            if not prediction:\n",
    "                prediction = self.default_grade\n",
    "                \n",
    "            predictions.append(prediction)\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train advanced rule-based classifiers for each subject\n",
    "math_classifier = AdvancedRuleClassifier(\"Math\")\n",
    "math_classifier.build(X_train, y_train_math)\n",
    "\n",
    "reading_classifier = AdvancedRuleClassifier(\"Reading\")\n",
    "reading_classifier.build(X_train, y_train_reading)\n",
    "\n",
    "writing_classifier = AdvancedRuleClassifier(\"Writing\")\n",
    "writing_classifier.build(X_train, y_train_writing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_math = math_classifier.predict(X_test)\n",
    "y_pred_reading = reading_classifier.predict(X_test)\n",
    "y_pred_writing = writing_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation for Math:\n",
      "Accuracy: 0.00\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Above Average       0.00      0.00      0.00       7.0\n",
      "      Average       0.00      0.00      0.00      13.0\n",
      "Below Average       0.00      0.00      0.00      19.0\n",
      "    Excellent       0.00      0.00      0.00       6.0\n",
      "      Failure       0.00      0.00      0.00     121.0\n",
      "         Good       0.00      0.00      0.00       9.0\n",
      "      Passing       0.00      0.00      0.00      19.0\n",
      "     Superior       0.00      0.00      0.00       6.0\n",
      "      Unknown       0.00      0.00      0.00       0.0\n",
      "\n",
      "     accuracy                           0.00     200.0\n",
      "    macro avg       0.00      0.00      0.00     200.0\n",
      " weighted avg       0.00      0.00      0.00     200.0\n",
      "\n",
      "Evaluation for Reading:\n",
      "Accuracy: 0.00\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Above Average       0.00      0.00      0.00      19.0\n",
      "      Average       0.00      0.00      0.00      20.0\n",
      "Below Average       0.00      0.00      0.00      19.0\n",
      "    Excellent       0.00      0.00      0.00       6.0\n",
      "      Failure       0.00      0.00      0.00     107.0\n",
      "         Good       0.00      0.00      0.00       7.0\n",
      "      Passing       0.00      0.00      0.00      16.0\n",
      "     Superior       0.00      0.00      0.00       6.0\n",
      "      Unknown       0.00      0.00      0.00       0.0\n",
      "\n",
      "     accuracy                           0.00     200.0\n",
      "    macro avg       0.00      0.00      0.00     200.0\n",
      " weighted avg       0.00      0.00      0.00     200.0\n",
      "\n",
      "Evaluation for Writing:\n",
      "Accuracy: 0.00\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Above Average       0.00      0.00      0.00       8.0\n",
      "      Average       0.00      0.00      0.00      15.0\n",
      "Below Average       0.00      0.00      0.00      25.0\n",
      "    Excellent       0.00      0.00      0.00       8.0\n",
      "      Failure       0.00      0.00      0.00     112.0\n",
      "         Good       0.00      0.00      0.00      11.0\n",
      "      Passing       0.00      0.00      0.00      14.0\n",
      "     Superior       0.00      0.00      0.00       7.0\n",
      "      Unknown       0.00      0.00      0.00       0.0\n",
      "\n",
      "     accuracy                           0.00     200.0\n",
      "    macro avg       0.00      0.00      0.00     200.0\n",
      " weighted avg       0.00      0.00      0.00     200.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_classifier(y_true, y_pred, subject):\n",
    "    print(f\"Evaluation for {subject}:\")\n",
    "\n",
    "    # Ensure y_true and y_pred are Pandas Series with the same dtype\n",
    "    y_pred = pd.Series(y_pred).astype(str).reset_index(drop=True)\n",
    "    y_true = pd.Series(y_true).astype(str).reset_index(drop=True)\n",
    "\n",
    "    # Fill NaN values\n",
    "    y_true = y_true.fillna(\"Unknown\")\n",
    "    y_pred = y_pred.fillna(\"Unknown\")\n",
    "\n",
    "    # Ensure predictions contain only known classes\n",
    "    valid_classes = set(y_true.unique())\n",
    "    y_pred = y_pred.apply(lambda x: x if x in valid_classes else \"Unknown\")\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.2f}\\n\")\n",
    "\n",
    "    # Print classification report\n",
    "    print(classification_report(y_true, y_pred, zero_division=0))\n",
    "\n",
    "# Evaluate the classifiers\n",
    "evaluate_classifier(y_test_math, y_pred_math, \"Math\")\n",
    "evaluate_classifier(y_test_reading, y_pred_reading, \"Reading\")\n",
    "evaluate_classifier(y_test_writing, y_pred_writing, \"Writing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, subject):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=set(y_true), yticklabels=set(y_true))\n",
    "    \n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.title(f\"Confusion Matrix for {subject}\")\n",
    "    plt.show()\n",
    "\n",
    "# Confusion matrices for each subject\n",
    "plot_confusion_matrix(y_test_math, y_pred_math, \"Math\")\n",
    "plot_confusion_matrix(y_test_reading, y_pred_reading, \"Reading\")\n",
    "plot_confusion_matrix(y_test_writing, y_pred_writing, \"Writing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
