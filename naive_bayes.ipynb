{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('cleaned_exams.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Grading function based on the grade\n",
    "def assign_grade(score):\n",
    "    if 97 <= score <= 100:\n",
    "        return 'Excellent'\n",
    "    elif 93 <= score <= 96:\n",
    "        return 'Superior'\n",
    "    elif 89 <= score <= 92:\n",
    "        return 'Good'\n",
    "    elif 85 <= score <= 88:\n",
    "        return 'Above Average'\n",
    "    elif 81 <= score <= 84:\n",
    "        return 'Average'\n",
    "    elif 77 <= score <= 80:\n",
    "        return 'Below Average'\n",
    "    elif 73 <= score <= 76:\n",
    "        return 'Passing'\n",
    "    else:\n",
    "        return 'Failure'\n",
    "\n",
    "# Assign grades to each subject\n",
    "df['math_grade'] = df['math score'].apply(assign_grade)\n",
    "df['reading_grade'] = df['reading score'].apply(assign_grade)\n",
    "df['writing_grade'] = df['writing score'].apply(assign_grade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def process_subject(subject):\n",
    "\n",
    "    # Prepare features and target\n",
    "    X = df.drop(columns=[f'{subject}_grade'])\n",
    "    X = pd.get_dummies(X, drop_first=True)  # One-hot encode categorical features\n",
    "    y = df[f'{subject}_grade']\n",
    "\n",
    "    # Encode target labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(y)\n",
    "\n",
    "    # Split 70% training, 20% testing, 10% unseen\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    X_test, X_unseen, y_test, y_unseen = train_test_split(X_temp, y_temp, test_size=1/3, random_state=42, stratify=y_temp)\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    X_unseen_scaled = scaler.transform(X_unseen)\n",
    "\n",
    "    # Train Naive Bayes with cross-validation\n",
    "    nb_model = GaussianNB()\n",
    "    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    cross_val_acc = cross_val_score(nb_model, X_train_scaled, y_train, cv=cv, scoring='accuracy')\n",
    "    print(f\"Cross-validation Accuracy (Mean) for {subject}: {np.mean(cross_val_acc):.4f}\")\n",
    "\n",
    "    # Train final model\n",
    "    nb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Predict on test set\n",
    "    y_test_pred = nb_model.predict(X_test_scaled)\n",
    "    y_test_proba = nb_model.predict_proba(X_test_scaled)\n",
    "\n",
    "    # Test set evaluation\n",
    "    print(f\"\\n{subject} Test Evaluation:\")\n",
    "    print(\"Test Accuracy:\", round(accuracy_score(y_test, y_test_pred), 4))\n",
    "    # print(\"Classification Report:\\n\", classification_report(y_test, y_test_pred))\n",
    "    print(\"Precision:\", round(precision_score(y_test, y_test_pred, average='weighted'), 4))\n",
    "    print(\"Recall:\", round(recall_score(y_test, y_test_pred, average='weighted'), 4))\n",
    "    print(\"ROC-AUC:\", round(roc_auc_score(y_test, y_test_proba, multi_class='ovr'), 4))\n",
    "\n",
    "    # Confusion matrix for test set\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(confusion_matrix(y_test, y_test_pred), annot=True, fmt='d', cmap='Greens')\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(f\"Confusion Matrix - {subject} Test Set\")\n",
    "    plt.show()\n",
    "\n",
    "    # Predict on unseen data\n",
    "    y_unseen_pred = nb_model.predict(X_unseen_scaled)\n",
    "    y_unseen_proba = nb_model.predict_proba(X_unseen_scaled)\n",
    "\n",
    "    # Unseen data evaluation\n",
    "    print(f\"\\n{subject} Unseen Data Evaluation:\")\n",
    "    print(\"Unseen Accuracy:\", round(accuracy_score(y_unseen, y_unseen_pred), 4))\n",
    "    # print(\"Classification Report:\\n\", classification_report(y_unseen, y_unseen_pred))\n",
    "    print(\"Unseen Precision:\", round(precision_score(y_unseen, y_unseen_pred, average='weighted'), 4))\n",
    "    print(\"Unseen Recall:\", round(recall_score(y_unseen, y_unseen_pred, average='weighted'), 4))\n",
    "    print(\"Unseen ROC-AUC:\", round(roc_auc_score(y_unseen, y_unseen_proba, multi_class='ovr'), 4))\n",
    "\n",
    "    # Confusion matrix for unseen data\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(confusion_matrix(y_unseen, y_unseen_pred), annot=True, fmt='d', cmap='Oranges')\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(f\"Confusion Matrix - {subject} Unseen Data\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Process each subject separately\n",
    "for subject in ['math', 'reading', 'writing']:\n",
    "    process_subject(subject)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
